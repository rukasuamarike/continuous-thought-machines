# -*- coding: utf-8 -*-
"""ctm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wzCKX_h4s2HjDb3kw14I3hsRpNy1V7er
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import butter, lfilter,filtfilt
from google.colab import drive
import json
from textgrid import TextGrid

mount = '/content/drive'
drive.mount(mount)
path_processing = "/content/drive/MyDrive/SD - Team 2024-2025/Data Visual & Modified"
dss = ["trial_3","trial_4"]
for ds in dss:
  with open(f"{path_processing}/ds_{ds}.json",'r') as f_load:
    d_trial = json.load(f_load)
    d[ds] = d_trial[ds]

d['trial_4'].keys() # dict_keys(['path', 'prompts', 'sr', 'emg_files', 'text_files', 'wav_files'])
phon = json.load(open(f"{path_processing}/charsiu_phoneme.json"))
phoneme_maps = {}
for idx,key in enumerate(phon.keys()):
  phoneme_maps[key]=idx


def textgrid_to_matrix(file,emg_data,emg_sr):
  ts_data = TextGrid.fromFile(file)
  out = np.zeros_like(emg_data[:,0])
  back = 0
  for interval in ts_data[0].intervals:
      xmin, xmax, text = interval.minTime, interval.maxTime, interval.mark
      for t in range(int(xmin * emg_sr), min(out.shape[0],int(xmax * emg_sr))):
        out[t] = phoneme_maps[text]
        back = t
  if(back<out.shape[0]):
    for i in range(back,len(out)):
      out[i] = phoneme_maps["[SIL]"]
  return out

def remove_spike(data,w=2,threshold=600):
    spike_indices = np.where(np.diff(data) > threshold)[0]
    for idx in spike_indices:
        if w < idx < len(data) - w:
            data[idx-w:idx+w] = np.repeat([(data[idx - w] + data[idx + w]) / 2],2*w)
    return data

def butter_bandpass(data,lowcut, highcut, fs, order=2):
    nyq = 0.5 * fs
    low = lowcut / nyq
    high = highcut / nyq
    b, a = butter(order, [low, high], btype='band')
    return filtfilt(b, a, data)


index = 3
name = 'trial_4'
data_file = d[name]["emg_files"][index]
label_file = d[name]["text_files"][index]
emg_sr = d[name]["sr"]
data = np.loadtxt(data_file,delimiter=',')
emg_data = data[:,1:5]
labels = textgrid_to_matrix(label_file,emg_data,emg_sr)

# cleaning
for ch in range(4):
        chan = emg_data[:,ch]
        chan[0] = chan[1]
        emg_data[:,ch] = remove_spike(chan)
        emg_data[:,ch] = butter_bandpass(emg_data[:,ch],0.5,50,emg_sr)

import numpy as np
import matplotlib.pyplot as plt
from scipy.signal import find_peaks, hilbert
from scipy import signal
import warnings
warnings.filterwarnings('ignore')

class OnsetDetector:
    def __init__(self,
                 threshold_percent=15,  # 15% of peak energy as mentioned in sources
                 min_distance_samples=50,  # Minimum samples between onsets (~200ms at 250Hz)
                 window_size_samples=125):  # ~500ms windows at 250Hz
        """
        MVP Onset Detection for EMG signals

        Args:
            threshold_percent: Percentage of peak energy for threshold
            min_distance_samples: Minimum distance between detected onsets
            window_size_samples: Size of analysis window around onsets
        """
        self.threshold_percent = threshold_percent
        self.min_distance = min_distance_samples
        self.window_size = window_size_samples

    def detect_channel_onsets(self, channel_data, sr=250):
        """
        Detect onsets in a single EMG channel using energy-based method
        """
        # Calculate signal energy (similar to papers mentioning energy-based detection)
        # Use Hilbert transform to get analytic signal envelope
        analytic_signal = hilbert(channel_data)
        envelope = np.abs(analytic_signal)

        # Smooth the envelope to reduce noise
        smoothed_envelope = signal.savgol_filter(envelope,
                                               window_length=min(51, len(envelope)//10),
                                               polyorder=3)

        # Calculate threshold as percentage of peak energy
        peak_energy = np.max(smoothed_envelope)
        threshold = (self.threshold_percent / 100.0) * peak_energy

        # Find peaks above threshold
        onsets, properties = find_peaks(smoothed_envelope,
                                      height=threshold,
                                      distance=self.min_distance)

        return {
            'onsets': onsets,
            'envelope': smoothed_envelope,
            'threshold': threshold,
            'peak_energy': peak_energy,
            'heights': properties['peak_heights'] if onsets.size > 0 else np.array([])
        }

    def detect_cross_channel_onsets(self, emg_data):
        """
        Detect onsets across all channels and create unified onset triggers

        Args:
            emg_data: Shape (samples, channels) - your 4-channel EMG data
        """
        n_samples, n_channels = emg_data.shape

        # Detect onsets in each channel
        channel_results = {}
        all_onsets = []

        for ch in range(n_channels):
            result = self.detect_channel_onsets(emg_data[:, ch])
            channel_results[f'ch_{ch}'] = result

            # Add channel info to onsets
            for onset in result['onsets']:
                all_onsets.append({
                    'sample': onset,
                    'channel': ch,
                    'strength': result['envelope'][onset]
                })

        # Sort all onsets by time
        all_onsets.sort(key=lambda x: x['sample'])

        # Create unified trigger points (remove duplicates within min_distance)
        unified_onsets = []
        if all_onsets:
            unified_onsets.append(all_onsets[0])

            for onset in all_onsets[1:]:
                # Check if this onset is far enough from the last unified onset
                if onset['sample'] - unified_onsets[-1]['sample'] >= self.min_distance:
                    unified_onsets.append(onset)

        return {
            'channel_results': channel_results,
            'unified_onsets': unified_onsets,
            'n_onsets': len(unified_onsets)
        }

    def segment_around_onsets(self, emg_data, onset_results):
        """
        Create time windows around detected onsets for CWT processing
        """
        segments = []
        n_samples = emg_data.shape[0]

        for onset_info in onset_results['unified_onsets']:
            onset_sample = onset_info['sample']

            # Define window around onset
            start_idx = max(0, onset_sample - self.window_size // 2)
            end_idx = min(n_samples, onset_sample + self.window_size // 2)

            # Extract segment
            segment = emg_data[start_idx:end_idx, :]

            segments.append({
                'data': segment,
                'onset_sample': onset_sample,
                'window_start': start_idx,
                'window_end': end_idx,
                'trigger_channel': onset_info['channel'],
                'trigger_strength': onset_info['strength']
            })

        return segments

# Test the MVP with your data
def test_onset_detection(emg_data, emg_sr=250):
    """
    Test function for the onset detection MVP
    """
    detector = OnsetDetector(
        threshold_percent=15,  # As mentioned in your sources
        min_distance_samples=int(0.2 * emg_sr),  # 200ms minimum distance
        window_size_samples=int(0.5 * emg_sr)    # 500ms windows
    )

    print(f"Testing onset detection on EMG data shape: {emg_data.shape}")
    print(f"Sampling rate: {emg_sr} Hz")

    # Detect onsets
    onset_results = detector.detect_cross_channel_onsets(emg_data)

    print(f"\nDetected {onset_results['n_onsets']} unified onsets")

    # Create segments
    segments = detector.segment_around_onsets(emg_data, onset_results)

    print(f"Created {len(segments)} segments for further processing")

    # Visualization
    fig, axes = plt.subplots(4, 1, figsize=(15, 10))
    fig.suptitle('EMG Onset Detection Results')

    time_axis = np.arange(emg_data.shape[0]) / emg_sr

    for ch in range(4):
        ax = axes[ch]

        # Plot original signal
        ax.plot(time_axis, emg_data[:, ch], 'b-', alpha=0.7, label='EMG Signal')

        # Plot envelope and threshold
        ch_result = onset_results['channel_results'][f'ch_{ch}']
        ax.plot(time_axis, ch_result['envelope'], 'r-', linewidth=2, label='Envelope')
        ax.axhline(y=ch_result['threshold'], color='g', linestyle='--', label='Threshold')

        # Mark channel-specific onsets
        if len(ch_result['onsets']) > 0:
            onset_times = ch_result['onsets'] / emg_sr
            ax.scatter(onset_times, ch_result['heights'],
                      color='red', s=50, zorder=5, label='Onsets')

        # Mark unified onsets that came from this channel
        for onset_info in onset_results['unified_onsets']:
            if onset_info['channel'] == ch:
                onset_time = onset_info['sample'] / emg_sr
                ax.axvline(x=onset_time, color='orange', linestyle='-',
                          linewidth=3, alpha=0.8, label='Unified Trigger')

        ax.set_ylabel(f'Channel {ch}')
        ax.legend()
        ax.grid(True, alpha=0.3)

    axes[-1].set_xlabel('Time (s)')
    plt.tight_layout()
    plt.show()

    return onset_results, segments

# Run the test with your data
onset_results, segments = test_onset_detection(emg_data, emg_sr)

from scipy import signal as scipy_signal
import pywt

def apply_basic_cwt_to_segments(segments, scales=None, wavelet='gaus3', sr=250):
    """
    Apply basic CWT to segmented EMG data
    This is a simplified version before implementing fCWT

    Args:
        segments: List of segment dictionaries from onset detection
        scales: CWT scales to use (will auto-generate if None)
        wavelet: Wavelet to use ('gaus3' as you mentioned)
        sr: Sampling rate
    """
    if scales is None:
        # Create scales roughly corresponding to 1-50 Hz
        frequencies = np.logspace(np.log10(1), np.log10(50), 20)
        scales = pywt.frequency2scale(wavelet, frequencies/sr)

    cwt_features = []

    for i, segment in enumerate(segments):
        segment_data = segment['data']  # Shape: (samples, 4_channels)
        n_samples, n_channels = segment_data.shape

        # Apply CWT to each channel
        channel_cwts = []
        for ch in range(n_channels):
            # CWT for this channel
            coefficients, freqs = pywt.cwt(segment_data[:, ch], scales, wavelet, 1/sr)

            # Convert to power (magnitude squared)
            power = np.abs(coefficients)**2
            channel_cwts.append(power)

        # Stack all channels: Shape (scales, samples, channels)
        cwt_tensor = np.stack(channel_cwts, axis=-1)

        cwt_features.append({
            'cwt_data': cwt_tensor,
            'scales': scales,
            'frequencies': freqs,
            'segment_info': segment,
            'shape': cwt_tensor.shape
        })

        print(f"Segment {i}: CWT shape {cwt_tensor.shape} "
              f"(scales x samples x channels)")

    return cwt_features

def visualize_cwt_features(cwt_features, segment_idx=0):
    """
    Visualize CWT features for a specific segment
    """
    if segment_idx >= len(cwt_features):
        print(f"Segment {segment_idx} not available")
        return

    cwt_data = cwt_features[segment_idx]['cwt_data']
    freqs = cwt_features[segment_idx]['frequencies']
    segment_info = cwt_features[segment_idx]['segment_info']

    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    axes = axes.flatten()

    for ch in range(4):
        ax = axes[ch]

        # Plot CWT coefficients for this channel
        im = ax.imshow(cwt_data[:, :, ch],
                      aspect='auto',
                      cmap='jet',
                      extent=[0, cwt_data.shape[1], freqs[-1], freqs[0]])

        ax.set_ylabel('Frequency (Hz)')
        ax.set_xlabel('Time (samples)')
        ax.set_title(f'Channel {ch} - CWT Power')
        plt.colorbar(im, ax=ax)

    plt.suptitle(f'CWT Features - Segment {segment_idx}\n'
                f'Triggered by Channel {segment_info["trigger_channel"]} '
                f'at sample {segment_info["onset_sample"]}')
    plt.tight_layout()
    plt.show()



# Apply CWT to your segments
print("Applying basic CWT to detected segments...")
cwt_features = apply_basic_cwt_to_segments(segments, wavelet='gaus3', sr=emg_sr)

print(f"\nGenerated CWT features for {len(cwt_features)} segments")

# Visualize the first segment
if len(cwt_features) > 0:
    visualize_cwt_features(cwt_features, segment_idx=0)

def prepare_features_for_ctm(cwt_features, target_shape=(32, 128)):
    """
    Prepare CWT features for CTM input
    This creates spatio-temporal features ready for the FeatureEncoder

    Args:
        cwt_features: List of CWT feature dictionaries
        target_shape: Target shape for each segment (freq_bins, time_steps)
    """
    ctm_ready_features = []

    for i, cwt_feat in enumerate(cwt_features):
        cwt_data = cwt_feat['cwt_data']  # (scales, samples, channels)

        # For each channel, resize/interpolate to target shape
        channel_features = []
        for ch in range(cwt_data.shape[2]):
            channel_cwt = cwt_data[:, :, ch]  # (scales, samples)

            # Resize to target shape using interpolation
            from scipy.ndimage import zoom

            scale_factor_freq = target_shape[0] / channel_cwt.shape[0]
            scale_factor_time = target_shape[1] / channel_cwt.shape[1]

            resized = zoom(channel_cwt,
                          (scale_factor_freq, scale_factor_time),
                          mode='nearest')

            channel_features.append(resized)

        # Stack channels: Shape (channels, freq_bins, time_steps)
        feature_tensor = np.stack(channel_features, axis=0)

        ctm_ready_features.append({
            'features': feature_tensor,
            'original_segment': cwt_feat['segment_info'],
            'shape': feature_tensor.shape
        })

        print(f"Segment {i}: Prepared features shape {feature_tensor.shape}")

    return ctm_ready_features

def create_dataset_summary(onset_results, segments, cwt_features, ctm_features):
    """
    Create a summary of the complete pipeline
    """
    print("="*60)
    print("EMG ONSET-TRIGGERED SEGMENTATION PIPELINE SUMMARY")
    print("="*60)

    print(f"ðŸ“Š ONSET DETECTION:")
    print(f"   â€¢ Total unified onsets detected: {onset_results['n_onsets']}")
    print(f"   â€¢ Onsets per channel:")
    for ch in range(4):
        ch_onsets = len(onset_results['channel_results'][f'ch_{ch}']['onsets'])
        print(f"     - Channel {ch}: {ch_onsets} onsets")

    print(f"\nðŸ”§ SEGMENTATION:")
    print(f"   â€¢ Total segments created: {len(segments)}")
    print(f"   â€¢ Segment window size: {segments[0]['data'].shape[0] if segments else 0} samples")

    print(f"\nðŸŒŠ CWT PROCESSING:")
    print(f"   â€¢ CWT applied to {len(cwt_features)} segments")
    if cwt_features:
        print(f"   â€¢ CWT tensor shape per segment: {cwt_features[0]['shape']}")
        print(f"   â€¢ Number of frequency scales: {cwt_features[0]['cwt_data'].shape[0]}")

    print(f"\nðŸŽ¯ CTM-READY FEATURES:")
    print(f"   â€¢ Features prepared for {len(ctm_features)} segments")
    if ctm_features:
        print(f"   â€¢ Feature tensor shape per segment: {ctm_features[0]['shape']}")
        print(f"   â€¢ Ready for FeatureEncoder input")

    print(f"\nâœ… PIPELINE STATUS: MVP Complete!")
    print(f"   Next steps: Integrate with CTM FeatureEncoder")

# Prepare features for CTM
print("Preparing features for CTM input...")
ctm_ready_features = prepare_features_for_ctm(cwt_features)

# Generate summary
create_dataset_summary(onset_results, segments, cwt_features, ctm_ready_features)

ts_data = TextGrid.fromFile(label_file)

len(ts_data[0])

def textgrid_to_segments(textgrid_file, segments, emg_sr):
    """
    Align phoneme labels with onset-triggered segments
    """
    ts_data = TextGrid.fromFile(textgrid_file)
    segment_labels = []

    for segment in segments:
        start_time = segment['window_start'] / emg_sr
        end_time = segment['window_end'] / emg_sr
        segment_center = (start_time + end_time) / 2

        # Find phoneme at segment center or most overlapping
        phoneme = find_dominant_phoneme(ts_data, start_time, end_time)
        segment_labels.append(phoneme_maps[phoneme])

    return segment_labels

# 1. Initialize detector
detector = OnsetDetector(threshold_percent=15,
                        min_distance_samples=int(0.1 * emg_sr),
                        window_size_samples=int(0.25 * emg_sr))

# 2. Detect onsets and create segments
onset_results = detector.detect_cross_channel_onsets(emg_data)
segments = detector.segment_around_onsets(emg_data, onset_results)

# 3. Apply CWT to segments
cwt_features = apply_basic_cwt_to_segments(segments, wavelet='gaus3', sr=emg_sr)

# 4. Prepare for CTM
ctm_ready_features = prepare_features_for_ctm(cwt_features)

import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange, repeat

class EMGSelfAttentionBackbone(nn.Module):
    """
    Custom backbone for EMG CWT features using self-attention across channels

    Input: CWT features of shape (batch, channels=4, freq_bins, time_steps)
    Output: d_input dimensional vector for CTM processing
    """

    def __init__(self,
                 n_channels=4,
                 freq_bins=32,
                 time_steps=128,
                 d_model=256,        # Internal embedding dimension
                 d_input=512,        # Output dimension for CTM
                 n_heads=8,          # Multi-head attention
                 n_layers=2,         # Number of self-attention layers
                 dropout=0.1):

        super().__init__()

        self.n_channels = n_channels
        self.freq_bins = freq_bins
        self.time_steps = time_steps
        self.d_model = d_model
        self.d_input = d_input

        # Step 1: Project each channel's CWT features to d_model
        self.channel_projection = nn.Linear(freq_bins * time_steps, d_model)

        # Step 2: Learnable channel embeddings (like positional embeddings)
        self.channel_embeddings = nn.Parameter(torch.randn(n_channels, d_model))

        # Step 3: Self-attention across channels
        self.attention_layers = nn.ModuleList([
            ChannelAttentionBlock(d_model, n_heads, dropout)
            for _ in range(n_layers)
        ])

        # Step 4: Final projection to CTM input dimension
        self.output_projection = nn.Sequential(
            nn.Linear(n_channels * d_model, d_model),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_model, d_input),
            nn.LayerNorm(d_input)
        )

        # Alternative: Use attention pooling instead of simple flattening
        self.use_attention_pooling = True
        if self.use_attention_pooling:
            self.attention_pool = AttentionPooling(d_model, d_input)

    def forward(self, cwt_features):
        """
        Args:
            cwt_features: (batch, channels, freq_bins, time_steps)
        Returns:
            features: (batch, d_input) for CTM processing
        """
        batch_size = cwt_features.shape[0]

        # Step 1: Flatten and project each channel
        # (batch, channels, freq_bins, time_steps) -> (batch, channels, freq_bins * time_steps)
        channel_features = rearrange(cwt_features, 'b c f t -> b c (f t)')

        # Project each channel to d_model dimension
        # (batch, channels, freq_bins * time_steps) -> (batch, channels, d_model)
        channel_embeddings = self.channel_projection(channel_features)

        # Step 2: Add learnable channel embeddings
        # Broadcast channel embeddings across batch
        channel_emb = repeat(self.channel_embeddings, 'c d -> b c d', b=batch_size)
        channel_embeddings = channel_embeddings + channel_emb

        # Step 3: Self-attention across channels
        # Treat channels as sequence tokens for self-attention
        attended_features = channel_embeddings
        for attention_layer in self.attention_layers:
            attended_features = attention_layer(attended_features)

        # Step 4: Pool to final representation
        if self.use_attention_pooling:
            # Use learned attention pooling
            features = self.attention_pool(attended_features)
        else:
            # Simple flattening + projection
            features = rearrange(attended_features, 'b c d -> b (c d)')
            features = self.output_projection(features)

        return features

class ChannelAttentionBlock(nn.Module):
    """Self-attention block for processing EMG channels"""

    def __init__(self, d_model, n_heads, dropout=0.1):
        super().__init__()

        self.self_attention = nn.MultiheadAttention(
            d_model, n_heads, dropout=dropout, batch_first=True
        )
        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, 4 * d_model),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(4 * d_model, d_model)
        )

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x):
        """
        Args:
            x: (batch, channels, d_model)
        Returns:
            output: (batch, channels, d_model)
        """
        # Self-attention across channels
        attended, _ = self.self_attention(x, x, x)
        x = self.norm1(x + self.dropout(attended))

        # Feed-forward
        ff_output = self.feed_forward(x)
        x = self.norm2(x + self.dropout(ff_output))

        return x

class AttentionPooling(nn.Module):
    """Attention-based pooling to aggregate channel information"""

    def __init__(self, d_model, d_output):
        super().__init__()

        self.attention_weights = nn.Linear(d_model, 1)
        self.output_projection = nn.Linear(d_model, d_output)
        self.layer_norm = nn.LayerNorm(d_output)

    def forward(self, x):
        """
        Args:
            x: (batch, channels, d_model)
        Returns:
            pooled: (batch, d_output)
        """
        # Compute attention weights for each channel
        weights = self.attention_weights(x)  # (batch, channels, 1)
        weights = F.softmax(weights, dim=1)   # Normalize across channels

        # Weighted average of channels
        pooled = torch.sum(x * weights, dim=1)  # (batch, d_model)

        # Final projection
        output = self.layer_norm(self.output_projection(pooled))

        return output

# CTM hyperparameters optimized for EMG speech recognition
ctm_config = {
    # Core architecture
    'D': 1024,              # Rich latent space for phoneme dynamics
    'M': 30,                # Memory for ~120ms of EMG history at 250Hz
    'T': 75,                # Allow variable compute for different phonemes

    # Input/output dimensions
    'd_input': 512,         # From your backbone
    'd_hidden': 64,         # NLM hidden dimension
    'n_heads': 8,           # Multi-head attention

    # Synchronization sampling
    'D_out': 512,           # Output synchronization dimension
    'D_action': 256,        # Action synchronization dimension
    'sampling_strategy': 'random',  # Random neuron pairing

    # Synapse model
    'synapse_depth': 8,     # U-Net depth for synaptic processing
    'dropout': 0.1,

    # Output projection
    'num_classes': 38,      # Your phoneme classes
}

def extract_phoneme_for_timewindow_center(textgrid_file, start_sample, end_sample, emg_sr, phoneme_maps):
    """
    Alternative: Extract phoneme at the center of the time window
    """
    from textgrid import TextGrid

    # Get center time of the window
    center_time = (start_sample + end_sample) / (2 * emg_sr)

    ts_data = TextGrid.fromFile(textgrid_file)

    # Find phoneme that contains the center point
    for interval in ts_data[0].intervals:
        if interval.minTime <= center_time <= interval.maxTime:
            return phoneme_maps.get(interval.mark, phoneme_maps.get("[SIL]", 0))

    # No phoneme found at center
    return phoneme_maps.get("[SIL]", 0)

def extract_phoneme_for_timewindow_onset_bias(textgrid_file, start_sample, end_sample, emg_sr, phoneme_maps):
    """
    Alternative: Bias towards phoneme that starts within the window (good for onset-triggered segments)
    """
    from textgrid import TextGrid

    start_time = start_sample / emg_sr
    end_time = end_sample / emg_sr

    ts_data = TextGrid.fromFile(textgrid_file)

    # First, look for phonemes that start within our window
    for interval in ts_data[0].intervals:
        if start_time <= interval.minTime <= end_time:
            return phoneme_maps.get(interval.mark, phoneme_maps.get("[SIL]", 0))

    return extract_phoneme_for_timewindow_center(textgrid_file, start_sample, end_sample, emg_sr, phoneme_maps)


def integrate_backbone_with_ctm_pipeline(ctm_ready_features, textgrid_file, emg_sr, phoneme_maps):
    """
    Updated integration function that includes phoneme labeling
    """

    # Initialize the backbone
    backbone = EMGSelfAttentionBackbone(
        n_channels=4,
        freq_bins=32,
        time_steps=128,
        d_model=256,
        d_input=512,
        n_heads=8,
        n_layers=2,
        dropout=0.1
    )

    # Process each segment
    processed_segments = []

    for segment_features in ctm_ready_features:
        # segment_features['features'] has shape (channels=4, freq_bins, time_steps)
        cwt_tensor = segment_features['features']

        # Add batch dimension and process
        cwt_batch = cwt_tensor.unsqueeze(0)  # (1, 4, freq_bins, time_steps)

        # Get CTM-ready features
        ctm_input = backbone(cwt_batch)  # (1, d_input=512)

        # Extract phoneme label for this segment
        segment_info = segment_features['original_segment']
        start_sample = segment_info['window_start']
        end_sample = segment_info['window_end']

        # Get phoneme label using one of the strategies above
        phoneme_label = extract_phoneme_for_timewindow_onset_bias(
            textgrid_file, start_sample, end_sample, emg_sr, phoneme_maps
        )

        processed_segments.append({
            'ctm_features': ctm_input.squeeze(0),  # (512,)
            'original_segment': segment_info,
            'phoneme_label': phoneme_label
        })

    return processed_segments

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from einops import rearrange, repeat

# First, let's create a test function for your backbone
def test_emg_backbone_on_cwt_data(ctm_ready_features, phoneme_maps, textgrid_file, emg_sr):
    """
    Test the EMG Self-Attention Backbone on your prepared CWT features

    Args:
        ctm_ready_features: List of feature dictionaries from your phase 1 pipeline
        phoneme_maps: Your phoneme mapping dictionary
        textgrid_file: Path to textgrid file for labeling
        emg_sr: EMG sampling rate
    """

    print("ðŸ§ª Testing EMG Self-Attention Backbone")
    print("=" * 50)

    # Initialize the backbone
    backbone = EMGSelfAttentionBackbone(
        n_channels=4,           # Your 4 EMG channels
        freq_bins=32,           # From your CWT processing
        time_steps=128,         # From your CWT processing
        d_model=256,            # Internal processing dimension
        d_input=512,            # Output for CTM
        n_heads=8,              # Multi-head attention
        n_layers=2,             # Depth of self-attention
        dropout=0.1
    )

    print(f"Backbone initialized with {sum(p.numel() for p in backbone.parameters())} parameters")

    if len(ctm_ready_features) == 0:
        print("âŒ No CWT features available for testing!")
        return None

    # Test 1: Single segment forward pass
    print(f"\nðŸ“Š Test 1: Single Segment Forward Pass")
    print("-" * 30)

    test_segment = ctm_ready_features[0]
    cwt_features = test_segment['features']  # Shape: (4, 32, 128)

    print(f"Input CWT features shape: {cwt_features.shape}")

    # Add batch dimension
    cwt_batch = torch.FloatTensor(cwt_features).unsqueeze(0)  # (1, 4, 32, 128)

    # Forward pass
    with torch.no_grad():
        output = backbone(cwt_batch)

    print(f"âœ… Output shape: {output.shape}")
    print(f"âœ… Output range: [{output.min():.3f}, {output.max():.3f}]")
    print(f"âœ… Output mean: {output.mean():.3f}")
    print(f"âœ… Output std: {output.std():.3f}")

    # Test 2: Batch processing
    print(f"\nðŸ“Š Test 2: Batch Processing")
    print("-" * 30)

    # Create a batch from multiple segments
    batch_size = min(8, len(ctm_ready_features))
    batch_features = []

    for i in range(batch_size):
        features = ctm_ready_features[i]['features']
        batch_features.append(torch.FloatTensor(features))

    batch_tensor = torch.stack(batch_features)  # (batch_size, 4, 32, 128)

    with torch.no_grad():
        batch_output = backbone(batch_tensor)

    print(f"âœ… Batch input shape: {batch_tensor.shape}")
    print(f"âœ… Batch output shape: {batch_output.shape}")
    print(f"âœ… Processing {batch_size} segments simultaneously")

    # Test 3: Gradient flow check
    print(f"\nðŸ“Š Test 3: Gradient Flow Check")
    print("-" * 30)

    # Create a simple test to ensure gradients flow
    backbone.train()

    # Simple regression target
    target = torch.randn(batch_size, 512)

    # Forward pass
    output = backbone(batch_tensor)

    # Simple MSE loss
    loss = F.mse_loss(output, target)

    # Backward pass
    loss.backward()

    # Check if gradients exist
    has_grads = []
    for name, param in backbone.named_parameters():
        if param.grad is not None:
            has_grads.append((name, param.grad.norm().item()))

    print(f"âœ… Loss computed: {loss.item():.4f}")
    print(f"âœ… Parameters with gradients: {len(has_grads)}")
    print(f"âœ… Average gradient norm: {np.mean([g for _, g in has_grads]):.6f}")

    # Test 4: Attention analysis
    print(f"\nðŸ“Š Test 4: Cross-Channel Attention Analysis")
    print("-" * 30)

    backbone.eval()

    # Hook to capture attention weights from the attention blocks
    attention_weights = []

    def attention_hook(module, input, output):
        # For MultiheadAttention, we need to manually compute attention
        # This is a simplified version - in practice you'd need to modify the backbone
        # to return attention weights
        pass

    with torch.no_grad():
        output = backbone(cwt_batch)

        # Analyze channel embeddings before attention
        channel_features = rearrange(cwt_batch, 'b c f t -> b c (f t)')
        channel_embeddings = backbone.channel_projection(channel_features)

        # Add channel embeddings
        channel_emb = repeat(backbone.channel_embeddings, 'c d -> b c d', b=1)
        channel_embeddings = channel_embeddings + channel_emb

        print(f"âœ… Channel embeddings shape: {channel_embeddings.shape}")

        # Compute pairwise similarities between channels
        similarities = torch.cosine_similarity(
            channel_embeddings[:, :, None, :],
            channel_embeddings[:, None, :, :],
            dim=-1
        )

        print(f"âœ… Cross-channel similarities:")
        for i in range(4):
            for j in range(4):
                sim = similarities[0, i, j].item()
                print(f"   Channel {i} <-> Channel {j}: {sim:.3f}")

    # Test 5: Compare attention pooling vs simple pooling
    print(f"\nðŸ“Š Test 5: Attention vs Simple Pooling")
    print("-" * 30)

    with torch.no_grad():
        # Test with and without attention pooling
        backbone_simple = EMGSelfAttentionBackbone(
            n_channels=4, freq_bins=32, time_steps=128,
            d_model=256, d_input=512, n_heads=8, n_layers=2, dropout=0.1
        )
        backbone_simple.use_attention_pooling = False

        output_attention = backbone(cwt_batch)
        output_simple = backbone_simple(cwt_batch)

        print(f"âœ… Attention pooling output range: [{output_attention.min():.3f}, {output_attention.max():.3f}]")
        print(f"âœ… Simple pooling output range: [{output_simple.min():.3f}, {output_simple.max():.3f}]")
        print(f"âœ… Difference magnitude: {(output_attention - output_simple).abs().mean():.4f}")

    # Test 6: Feature variability across segments
    print(f"\nðŸ“Š Test 6: Feature Variability Across Segments")
    print("-" * 30)

    if len(ctm_ready_features) >= 5:
        # Process several segments and analyze variability
        outputs = []

        with torch.no_grad():
            for i in range(min(10, len(ctm_ready_features))):
                features = torch.FloatTensor(ctm_ready_features[i]['features']).unsqueeze(0)
                output = backbone(features)
                outputs.append(output.squeeze(0))

        outputs_tensor = torch.stack(outputs)  # (n_segments, 512)

        # Compute statistics
        feature_means = outputs_tensor.mean(dim=0)
        feature_stds = outputs_tensor.std(dim=0)

        print(f"âœ… Processed {len(outputs)} segments")
        print(f"âœ… Feature dimensionality: {outputs_tensor.shape[1]}")
        print(f"âœ… Average feature std: {feature_stds.mean():.4f}")
        print(f"âœ… Feature std range: [{feature_stds.min():.4f}, {feature_stds.max():.4f}]")

        # Plot feature statistics
        plt.figure(figsize=(12, 4))

        plt.subplot(1, 2, 1)
        plt.hist(feature_means.numpy(), bins=30, alpha=0.7)
        plt.title('Distribution of Feature Means')
        plt.xlabel('Mean Value')
        plt.ylabel('Count')

        plt.subplot(1, 2, 2)
        plt.hist(feature_stds.numpy(), bins=30, alpha=0.7)
        plt.title('Distribution of Feature Standard Deviations')
        plt.xlabel('Std Value')
        plt.ylabel('Count')

        plt.tight_layout()
        plt.show()

    print(f"\nðŸŽ‰ Backbone Testing Complete!")
    print("=" * 50)

    return {
        'backbone': backbone,
        'test_output': output,
        'batch_output': batch_output if 'batch_output' in locals() else None,
        'feature_stats': {
            'means': feature_means if 'feature_means' in locals() else None,
            'stds': feature_stds if 'feature_stds' in locals() else None
        }
    }

# Integration test with your existing pipeline
def test_full_pipeline_integration(emg_data, emg_sr, phoneme_maps, textgrid_file):
    """
    Test the complete pipeline: Onset Detection -> CWT -> Backbone
    """
    print("ðŸ”— Testing Full Pipeline Integration")
    print("=" * 50)

    # Use your existing onset detection pipeline
    detector = OnsetDetector(
        threshold_percent=15,
        min_distance_samples=int(0.2 * emg_sr),
        window_size_samples=int(0.5 * emg_sr)
    )

    # 1. Detect onsets and create segments
    onset_results = detector.detect_cross_channel_onsets(emg_data)
    segments = detector.segment_around_onsets(emg_data, onset_results)
    print(f"âœ… Created {len(segments)} segments")

    # 2. Apply CWT to segments
    cwt_features = apply_basic_cwt_to_segments(segments, wavelet='gaus3', sr=emg_sr)
    print(f"âœ… Generated CWT features for {len(cwt_features)} segments")

    # 3. Prepare features for CTM
    ctm_ready_features = prepare_features_for_ctm(cwt_features)
    print(f"âœ… Prepared {len(ctm_ready_features)} CTM-ready features")

    # 4. Test backbone
    test_results = test_emg_backbone_on_cwt_data(
        ctm_ready_features, phoneme_maps, textgrid_file, emg_sr
    )

    print(f"\nðŸŽ¯ Pipeline Summary:")
    print(f"   â€¢ EMG shape: {emg_data.shape}")
    print(f"   â€¢ Onsets detected: {onset_results['n_onsets']}")
    print(f"   â€¢ Segments created: {len(segments)}")
    print(f"   â€¢ Features processed: {len(ctm_ready_features)}")
    print(f"   â€¢ Backbone output shape: {test_results['test_output'].shape}")

    return test_results

# Quick test function
def quick_backbone_test(ctm_ready_features):
    """
    Quick test if you just want to verify the backbone works
    """
    if len(ctm_ready_features) == 0:
        print("No features to test!")
        return

    backbone = EMGSelfAttentionBackbone(
        n_channels=4, freq_bins=32, time_steps=128,
        d_model=256, d_input=512, n_heads=8, n_layers=2, dropout=0.1
    )

    # Test single forward pass
    test_features = torch.FloatTensor(ctm_ready_features[0]['features']).unsqueeze(0)

    with torch.no_grad():
        output = backbone(test_features)

    print(f"âœ… Quick test passed!")
    print(f"   Input: {test_features.shape}")
    print(f"   Output: {output.shape}")
    print(f"   Parameters: {sum(p.numel() for p in backbone.parameters())}")

    return backbone, output

# Full comprehensive test
test_results = test_emg_backbone_on_cwt_data(
    ctm_ready_features=ctm_ready_features,
    phoneme_maps=phoneme_maps,  # Your existing phoneme mapping
    textgrid_file=label_file,   # Your textgrid file
    emg_sr=emg_sr              # Your sampling rate
)

# Or test the complete pipeline end-to-end
full_test_results = test_full_pipeline_integration(
    emg_data=emg_data,
    emg_sr=emg_sr,
    phoneme_maps=phoneme_maps,
    textgrid_file=label_file
)

# Or just a quick test
# backbone, output = quick_backbone_test(ctm_ready_features)

detector = OnsetDetector(
      threshold_percent=15,
      min_distance_samples=int(0.2 * emg_sr),
      window_size_samples=int(0.5 * emg_sr)
  )

# 1. Detect onsets and create segments
onset_results = detector.detect_cross_channel_onsets(emg_data)
segments = detector.segment_around_onsets(emg_data, onset_results)
print(f"âœ… Created {len(segments)} segments")

# 2. Apply CWT to segments
cwt_features = apply_basic_cwt_to_segments(segments, wavelet='gaus3', sr=emg_sr)
print(f"âœ… Generated CWT features for {len(cwt_features)} segments")

# 3. Prepare features for CTM
ctm_ready_features = prepare_features_for_ctm(cwt_features)
print(f"âœ… Prepared {len(ctm_ready_features)} CTM-ready features")

# # 4. Test backbone
# test_results = test_emg_backbone_on_cwt_data(
#     ctm_ready_features, phoneme_maps, label_file, emg_sr
# )
def test_backbone_ctm_integration(ctm_ready_features):
    """Test your backbone output with CTM input"""

    # Your backbone produces perfect (batch, 512) features
    # CTM expects exactly this format

    print("âœ… Backboneâ†’CTM integration ready!")
    print(f"âœ… Feature shape: {ctm_ready_features[0]['features'].shape}")
    print(f"âœ… Feature range: [{ctm_ready_features[0]['features'].min():.3f}, "
          f"{ctm_ready_features[0]['features'].max():.3f}]")
test_backbone_ctm_integration(ctm_ready_features)

